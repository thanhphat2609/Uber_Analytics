{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654e6fe0",
   "metadata": {},
   "source": [
    "# 1. Import needed libraries #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9bf049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, when, row_number, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd04278",
   "metadata": {},
   "source": [
    "# 2. Create Spark Session #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3abde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 00:36:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ET_Spark\") \\\n",
    "                            .config('spark.cores.max', \"16\") \\\n",
    "                            .config(\"spark.executor.memory\", \"70g\") \\\n",
    "                            .config(\"spark.driver.memory\", \"50g\") \\\n",
    "                            .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "                            .config(\"spark.memory.offHeap.size\",\"16g\") \\\n",
    "                            .config(\"spark.sql.warehouse.dir\", \"hdfs://localhost:9000/user/hive/warehouse\")\\\n",
    "                            .enableHiveSupport() \\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273cd097",
   "metadata": {},
   "source": [
    "# 3. Create functions for Dimension and Fact #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cdd253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for Extract data from HDFS and convert to RDD\n",
    "def extract_data(hdfs_path):\n",
    "    dataframe = spark.read.csv(hdfs_path, header = True, inferSchema = True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c75d3dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|  pickup_longitude|   pickup_latitude|RatecodeID|store_and_fwd_flag| dropoff_longitude|  dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|       1| 2016-03-01 00:00:19|  2016-03-01 00:18:02|              1|          7.7|-74.01290130615233| 40.70807266235352|         1|                 N|-73.96126556396483| 40.64998245239258|           1|       23.5|  0.5|    0.5|       1.0|         0.0|                  0.3|        25.8|\n",
      "|       2| 2016-03-01 00:00:32|  2016-03-01 00:08:59|              1|         2.46|-73.97491455078125| 40.76174545288086|         1|                 N| -73.9890899658203|40.732303619384766|           1|        9.5|  0.5|    0.5|       1.0|         0.0|                  0.3|        11.8|\n",
      "|       2| 2016-03-10 07:09:22|  2016-03-10 07:20:38|              1|         1.37|      -73.96484375| 40.75577926635742|         1|                 N|-73.98269653320312| 40.76639175415039|           1|        9.0|  0.0|    0.5|      1.96|         0.0|                  0.3|       11.76|\n",
      "|       2| 2016-03-10 07:10:25|  2016-03-10 07:19:30|              1|          1.2| -74.0048599243164|40.737674713134766|         1|                 N|-73.99419403076173| 40.72831344604492|           1|        7.5|  0.0|    0.5|       1.0|         0.0|                  0.3|         9.3|\n",
      "|       2| 2016-03-10 07:12:16|  2016-03-10 07:28:38|              5|         2.56| -73.9735336303711|40.786991119384766|         1|                 N|-73.99043273925781| 40.75632095336913|           1|       13.0|  0.0|    0.5|       2.0|         0.0|                  0.3|        15.8|\n",
      "|       2| 2016-03-10 07:13:14|  2016-03-10 07:20:00|              1|         1.47|-73.97786712646483| 40.78694152832031|         1|                 N|-73.98699188232422| 40.76999282836913|           1|        7.5|  0.0|    0.5|      2.08|         0.0|                  0.3|       10.38|\n",
      "|       2| 2016-03-10 07:14:48|  2016-03-10 07:31:24|              1|         6.58|-74.01612854003906| 40.71525955200195|         1|                 N|-73.97753143310547|  40.7522087097168|           2|       20.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        21.3|\n",
      "|       2| 2016-03-10 07:15:21|  2016-03-10 07:23:04|              1|         1.49|-73.95903778076173|40.763648986816406|         1|                 N|-73.97457122802734| 40.75709915161133|           1|        7.5|  0.0|    0.5|      1.25|         0.0|                  0.3|        9.55|\n",
      "|       2| 2016-03-10 07:16:22|  2016-03-10 07:38:28|              1|         6.23|-74.00849914550781| 40.70391845703125|         1|                 N|-73.98457336425781|40.754920959472656|           1|       21.0|  0.0|    0.5|       4.0|         0.0|                  0.3|        25.8|\n",
      "|       2| 2016-03-10 07:16:45|  2016-03-10 07:28:55|              2|         1.37| -73.9798126220703| 40.74932861328125|         1|                 N|-73.98748779296875| 40.76198959350586|           2|        9.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         9.8|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read data from HDFS\n",
    "hdfs_path = \"hdfs://localhost:9000/user/thanhphat/datalake/uberdata/*.csv\"\n",
    "df_uber = extract_data(hdfs_path)\n",
    "\n",
    "# Drop duplicates data\n",
    "df_uber = df_uber.dropDuplicates()\n",
    "\n",
    "# Show 10 rows data from the begining\n",
    "df_uber.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07025b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_datetime(df_base):\n",
    "    # Choose column\n",
    "    dim_datetime_df = df_base['tpep_pickup_datetime','tpep_dropoff_datetime']\n",
    "    \n",
    "    # Make an identiy\n",
    "    w = Window().orderBy(lit('A'))\n",
    "    \n",
    "    # Create DateTimeID for Dim_DateTime\n",
    "    dim_datetime_df = dim_datetime_df.withColumn('DateTimeId', row_number().over(w))\n",
    "    \n",
    "    # Move DateTimeID to the first column\n",
    "    new_order_columns = [\"DateTimeId\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "    dim_datetime_df = dim_datetime_df.select(*[col(c) for c in new_order_columns])\n",
    "    \n",
    "    return dim_datetime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c089b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 00:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------------+\n",
      "|DateTimeId|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+----------+--------------------+---------------------+\n",
      "|         1| 2016-03-01 00:00:19|  2016-03-01 00:18:02|\n",
      "|         2| 2016-03-01 00:00:32|  2016-03-01 00:08:59|\n",
      "|         3| 2016-03-10 07:09:22|  2016-03-10 07:20:38|\n",
      "|         4| 2016-03-10 07:10:25|  2016-03-10 07:19:30|\n",
      "|         5| 2016-03-10 07:12:16|  2016-03-10 07:28:38|\n",
      "+----------+--------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_datetime_df = create_dim_datetime(df_uber)\n",
    "dim_datetime_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14eabd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for create Dim_Pickup\n",
    "def create_dim_pickup(df_base):\n",
    "    # Change error column name\n",
    "    df_base = df_base.withColumnRenamed(\"pickup_longitude\", \"pickup_longtitude\")\n",
    "    \n",
    "    # Choose column\n",
    "    dim_pickup_df = df_base['pickup_longtitude', 'pickup_latitude']\n",
    "    \n",
    "    # Make an identity\n",
    "    w = Window().orderBy(lit('A'))\n",
    "    \n",
    "    # Create PickUpID for Dim_Pickup\n",
    "    dim_pickup_df = dim_pickup_df.withColumn('PickUpID', row_number().over(w))\n",
    "    \n",
    "    # Change column to the first\n",
    "    new_order = [\"PickUpID\", \"pickup_longtitude\", \"pickup_latitude\"]\n",
    "    dim_pickup_df = dim_pickup_df.select(*[col(c) for c in new_order])\n",
    "    \n",
    "    return dim_pickup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7744b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for create Dim_Dropoff\n",
    "def create_dim_dropoff(df_base):\n",
    "    # Change error column name\n",
    "    df_base = df_base.withColumnRenamed(\"dropoff_longitude\", \"dropoff_longtitude\")\n",
    "    \n",
    "    # Choose column\n",
    "    dim_dropoff_df = df_base['dropoff_longtitude', 'dropoff_latitude']\n",
    "    \n",
    "    # Make an identity\n",
    "    w = Window().orderBy(lit('A'))\n",
    "    \n",
    "    # Create DropOffID for Dim_DropOff\n",
    "    dim_dropoff_df = dim_dropoff_df.withColumn('DropOffID', row_number().over(w))\n",
    "    \n",
    "    # Change column to the first\n",
    "    new_order = ['DropOffID', \"dropoff_longtitude\", \"dropoff_latitude\"]\n",
    "    dim_dropoff_df = dim_dropoff_df.select(*[col(c) for c in new_order])\n",
    "    \n",
    "    return dim_dropoff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef1dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim_Pickup\n",
    "dim_pickup_df = create_dim_pickup(df_uber)\n",
    "\n",
    "# Dim_Dropoff\n",
    "dim_dropoff_df = create_dim_dropoff(df_uber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc08aa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "|PickUpID| pickup_longtitude|   pickup_latitude|\n",
      "+--------+------------------+------------------+\n",
      "|       1|-74.01290130615233| 40.70807266235352|\n",
      "|       2|-73.97491455078125| 40.76174545288086|\n",
      "|       3|      -73.96484375| 40.75577926635742|\n",
      "|       4| -74.0048599243164|40.737674713134766|\n",
      "|       5| -73.9735336303711|40.786991119384766|\n",
      "+--------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "dim_pickup_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21df09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/31 00:36:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+\n",
      "|DropOffID|dropoff_longtitude|  dropoff_latitude|\n",
      "+---------+------------------+------------------+\n",
      "|        1|-73.96126556396483| 40.64998245239258|\n",
      "|        2| -73.9890899658203|40.732303619384766|\n",
      "|        3|-73.98269653320312| 40.76639175415039|\n",
      "|        4|-73.99419403076173| 40.72831344604492|\n",
      "|        5|-73.99043273925781| 40.75632095336913|\n",
      "+---------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_dropoff_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d08f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for mapping dict data\n",
    "def translate(mapping):\n",
    "    def translate_(col):\n",
    "        return mapping.get(col)\n",
    "    return udf(translate_, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d35cbd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for create Dim_RateCode\n",
    "def create_dim_ratecode(df_base):\n",
    "    # Choose column\n",
    "    dim_ratecode_df = df_base[['RatecodeID']]\n",
    "    \n",
    "    # Drop duplicates\n",
    "    dim_ratecode_df = dim_ratecode_df.dropDuplicates()\n",
    "    \n",
    "    # Order data\n",
    "    dim_ratecode_df = dim_ratecode_df.orderBy(\"RatecodeID\")\n",
    "    \n",
    "    # Make a list type of ratecode\n",
    "    dict_ratecode_type = {\n",
    "        1:\"Standard rate\",\n",
    "        2:\"JFK\",\n",
    "        3:\"Newark\",\n",
    "        4:\"Nassau or Westchester\",\n",
    "        5:\"Negotiated fare\",\n",
    "        6:\"Group ride\"\n",
    "    }\n",
    "    \n",
    "    # Map data from dict_ratecode_name\n",
    "    dim_ratecode_df = dim_ratecode_df.withColumn(\"rate_code_type\", translate(dict_ratecode_name)(\"RatecodeID\"))\n",
    "    \n",
    "    return dim_ratecode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ff8ce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|RatecodeID|      rate_code_name|\n",
      "+----------+--------------------+\n",
      "|         1|       Standard rate|\n",
      "|         2|                 JFK|\n",
      "|         3|              Newark|\n",
      "|         4|Nassau or Westche...|\n",
      "|         5|     Negotiated fare|\n",
      "|         6|          Group ride|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_ratecode_df = create_dim_ratecode(df_uber)\n",
    "dim_ratecode_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3572b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for create Dim_Payment\n",
    "def create_dim_payment(df_base):\n",
    "    # Choose colum\n",
    "    dim_payment_df = df_base[['payment_type']]\n",
    "    \n",
    "    # Drop duplicates\n",
    "    dim_payment_df = dim_payment_df.dropDuplicates()\n",
    "    \n",
    "    # Order data\n",
    "    dim_payment_df = dim_payment_df.orderBy(\"payment_type\")\n",
    "    \n",
    "    # Dict name of payment_type\n",
    "    dict_payment_type_name = {\n",
    "        1:\"Credit card\",\n",
    "        2:\"Cash\",\n",
    "        3:\"No charge\",\n",
    "        4:\"Dispute\"\n",
    "    }\n",
    "    \n",
    "    # Map data\n",
    "    dim_payment_df = dim_payment_df.withColumn(\"payment_type_name\", \n",
    "                                               translate(dict_payment_type_name)(\"payment_type\"))\n",
    "    \n",
    "    return dim_payment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f9156ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|payment_type|payment_type_name|\n",
      "+------------+-----------------+\n",
      "|           1|      Credit card|\n",
      "|           2|             Cash|\n",
      "|           3|        No charge|\n",
      "|           4|          Dispute|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_payment_df = create_dim_payment(df_uber)\n",
    "dim_payment_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51741be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for create Fact_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb569a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
